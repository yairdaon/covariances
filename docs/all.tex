
\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{cite}
\usepackage{graphicx}
\usepackage{algorithm} % algorithm package
\usepackage[noend]{algpseudocode}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text





%----------------------------------------------------------------------------------------
% new commands
%----------------------------------------------------------------------------------------
\newcommand{\der}{\text{d}}
\newcommand{\coder}[1]{\texttt{#1}}
\newcommand{\inner}[2]{#1 \cdot #2}
\newcommand{\Op}{\mathcal{L}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{Corr}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\diag}{\text{diag}}

\title{Title TBD}

\author{Yair Daon}
\date{}

\pdfinfo{%
  /Title    ()
  /Author   (Yair Daon)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Problem statement}
Say one has a domain $\Omega \subseteq \mathbb{R}^d$. One would like to consider a covariance operator on the domain 
that does not ``see'' the boundary. This means that there are no strong correlations / decorrelations close to
the boundary. The covariances that are considered in this context are ``laplacian-like operators''. Their properties
are studied by \cite{stuart2010inverse}.
We try to take $(-\Delta + \kappa^2 )^{-p}$, where $\kappa^2 > 0$
and $p$ staisfies the restriction stated by \cite{stuart2010inverse}. Usually $p > \frac{d}{2}$,
where $d$ is the dimensionality defined above. If one takes Dirichlet boundary conditions, one obeserves 
strong decorrelations for points near the boundary. For Neumann boundary, the opposite occurs. We try 
to remove or ameliorate these boundary effects \cite{bui2013computational}.

Below I consider several approaches to this problem.

\section{Preliminaries}

Throught this paper, the term fundamental solution is reserved for the free space Green's 
function of a differential operator. The term Green's function 
is reserved exclusively for the one arising from a differential
operator on a domain with prescribed boundary conditions. This operator
may be denoted $\Op$.

% \subsection{Fundamental Solution and Green's Funciton}
% Fix $\kappa,d, y\in \Omega$ and $\beta > 0$. Denote $G$ the Green's function. 
% \begin{align}
%   \begin{split}
%     &(-\Delta_{x} + \kappa^2 ) G(x,y) = \delta_{y}(x),  x \in  \Omega, \\
%     &\beta G(x,y) + \frac{\partial G}{\partial n_{x}}(x,y) = 0, x \in \partial \Omega, 
%   \end{split}
% \end{align}

% $G$ can be shown to be symmetric. With the same notation, define the fundamental solution

% \begin{align}
%   \begin{split}
%     (-\Delta_{x} + \kappa^2) \Phi(x,y) &= \delta_{y}(x), x \in \mathbb{R}^d. \\
%   \end{split}
% \end{align}

% It will usually be clear from the context which operator is being used.

\subsection{Fundamental Solutions for $d = 1,2,3$}

It is known that:
\begin{itemize}
\item If $d = 1$, then $\Phi(x,y) = \frac{ \exp( -\kappa||x-y|| )}{2\kappa}$. To see this, take WLOG $y=0$ and $\Phi(x) = \frac{\exp(-\kappa |x| )}{2\kappa}$. 
  \begin{align}
    \begin{split}
      \kappa \frac{d^{2}m}{dx^2} &= \frac{d}{dx} [-\kappa H(x) \exp( -\kappa |x|)] \\
      %
      %
      %
      & = (-\kappa H(x))^2 \exp( -\kappa |x| ) -\kappa \exp(-\kappa |x| ) \delta (x) \\ 
      % 
      %
      %
      & = \kappa^2 \exp( -\kappa |x| ) -\kappa \exp(-\kappa |x| ) \delta (x) \\   
    \end{split}
  \end{align}
  and it is easy to see now that $(-\Delta + \kappa^2) \Phi(x) = \delta (x)$. 
\item If $d = 2$, then $\Phi(x,y) = \frac{1}{2\pi} K_{0}( \kappa ||x-y|| ) $ \cite{wikifundamental}. By
  \cite[page 79, number (7)]{watson1995treatise}, $\frac{dK_{0}(x)}{dx} = -K_{1}(x) = -K_{-1}(x)$. Also
  \begin{align*}
    \nabla_{x} \Phi(x,y) &= \nabla_{x} \frac{1}{2\pi} K_{0}( \kappa ||x-y|| )\\
    &= -\frac{\kappa}{2\pi} K_{1}( \kappa ||x-y|| ) \nabla_{x}||x-y|| \\
    &= -\frac{\kappa}{2\pi} K_{1}(\kappa ||x-y|| ) \frac{x-y}{||x-y||}
  \end{align*}
  \item If $d = 3$, then $\Phi(x,y) = \frac{ \exp( -\kappa||x-y|| )}{4\pi||x-y||}$ \cite{wikifundamental}.
\end{itemize}

\subsection{Matern as a Fundamental Solution}
It is known \cite{lindgren2011explicit} that the fundamental solution in $\mathbb{R}^d$ of

$$
(-\Delta + \kappa^2)^{ \nu + d/2 }
$$

is the Matern covariance function:

$$
m(x,y; \kappa, \nu ) = \frac{\sigma^2}{2^{\nu-1}\Gamma(\nu)} (\kappa||x-y||)^{\nu} K_{\nu}( \kappa||x-y||), 
$$

with $\nu, \kappa > 0$ and

$$
\sigma^2 = \frac{\Gamma(\nu)}{\Gamma(\nu + d/2) (4\pi)^{d/2} \kappa^{2\nu}}.
$$

The following identity

\begin{align}\label{diff}
  \frac{\text{d}}{\text{d}x} (x^{\nu} K_{\nu}(x)) &= -x^{\nu}K_{\nu-1}(x), \ \forall \nu \in \mathbb{Z},
\end{align}

facilitates calculation of the gradient whenever $\nu \in \mathbb{Z}$:
\begin{align}\label{grad_matern}
  \begin{split}
  \nabla_{x} m( x,y; \kappa, \nu ) &= \nabla_{x} \frac{\sigma^2}{2^{\nu-1}\Gamma(\nu)} (\kappa||x-y||)^{\nu}K_{\nu}(\kappa||x-y||) \\
  %
  %
  %
  &= -\frac{\sigma^2}{2^{\nu-1}\Gamma(\nu)} (\kappa||x-y||)^{\nu}K_{\nu-1}(\kappa||x-y||) \nabla_{x} \kappa||x-y|| \\
  % 
  %
  %
  &= -\kappa\frac{\sigma^2}{2^{\nu-1}\Gamma(\nu)} (\kappa||x-y||)^{\nu}K_{\nu-1}(\kappa||x-y||) \frac{x-y}{||x-y||} \\
  \end{split}
\end{align}



\subsection{Representation Formula}

Here we show how Green's function is used to solve a differential equation. The following identity
is useful.
\begin{align}\label{modified green}
  \begin{split}
    \int_{\Omega} u( \kappa^2 - \Delta) v - v( \kappa^2 - \Delta) u &= \int_{\Omega} v\Delta u - u\Delta v\\
    % 
    % 
    % 
    &= \int_{\partial \Omega } v\frac{\partial u}{\partial n} - u \frac{\partial v}{\partial n}.\\
  \end{split}
\end{align}

We want to find the solution to

\begin{align*}
  &(-\Delta + \kappa^2 ) u = f \text{ in } \Omega, \\
  &\beta u + \frac{\partial u}{\partial n}=0  \text{ in } \partial \Omega. 
\end{align*}

Fix $y\in \Omega$. From the modified Green's identity \ref{modified green}:

\begin{align*}
    u(y) &= \int_{\Omega} u(x) \delta_y(x) dx \\
    % 
    % 
    % 
    &= \int_{\Omega}  u(x)(-\Delta_{x} +\kappa^2 )G(x,y) dx \\
    % 
    % 
    % 
    & = \int_{\Omega} G(x,y) f(x) dx + \int_{\partial\Omega} G(x,y) \frac{\partial u}{\partial n_{x}} - u(x) \frac{\partial G(x,y)}{\partial n_{x}} dS(x) \\
    % 
    % 
    % 
    & = \int_{\Omega} G(x,y) f(x) dx + \int_{\partial\Omega}   G(x,y) (-\beta u(x)) - u(x) \frac{\partial G(x,y)}{\partial n_{x}} dS(x) \\
    % 
    % 
    % 
    & = \int_{\Omega} G(x,y) f(x) dx - \int_{\partial\Omega} u(x) [ \beta  G(x,y)  + \frac{\partial G(x,y)}{\partial n_{x}} ] dS(x) \\
    %
    %
    % 
    & = \int_{\Omega} G(x,y) f(x) dx,
\end{align*}

which is the required representation formula. The third equality is sometimes called the Kirchhoff- Helmholtz representation.
This is indeed the solution we saught: 
\begin{align}
  \begin{split}
    %
    % 
    % 
    (-\Delta_{y} + \kappa^2 ) u(y) &= \int_{\Omega} (-\Delta_{y} +\kappa^2 )G(x,y)f(x)dx \\
    %
    % 
    % 
    & = \int_{\Omega} \delta_{x}(y) f(x)dx \\
    %
    % 
    % 
    & = \int_{\Omega} \delta_{0}(x-y) f(x)dx \\ 
    % 
    % 
    % 
    & = f(y) \text{ for } y \in \Omega\\
    %
    % 
    % 
    \beta u(y) & =\int_{\Omega}  \beta G(x,y)f(x)dx \text{ (now } y\in \partial \Omega).\\
    %
    % 
    % 
    & =- \int_{\Omega} \frac{\partial G}{\partial n_{y}}f(x)dx \\
    %
    % 
    % 
    & =- \frac{\partial}{\partial n_{y}}  \int_{\Omega} G(x,y)f(x)dx \\
    %
    % 
    % 
    & =- \frac{\partial u} {\partial n}(y) 
  \end{split}
\end{align}


\subsection{Variational Formulation}
When using Finite Element, one needs to derive a variational formulation for the problem they are using. It is important 
to check that this variational formulation makes sense from an optimization perspective. This is the 
goal of this section. We show how a Robin problem arises from a minimization problem.
Take $I[u] = \frac{1}{2}\int_{\Omega} ||\nabla u||^{2} + \frac{1}{2}\kappa^{2}u^2 - fu + \frac{1}{2}\int_{\partial \Omega} \beta( u-\frac{g}{\beta})^2, \beta > 0$ and define
$i(\tau) = I[u + \tau v]$, with $u,v \in H^{1}(\Omega)$. Note that if $\beta < 0$ then $I$ is not bounded below, so the minimization
problem we consider shortly makes no sense.

\begin{align*}
  i(\tau) &= I[u + \tau v ] \\
  %
  %
  % 
  &= \frac{1}{2}\int_{\Omega} ||\nabla (u+ \tau v)||^{2} + \frac{1}{2}\kappa^{2}(u+ \tau v)^2 - f(u + \tau v) + \frac{1}{2}\int_{\partial \Omega} \beta(u- \frac{g}{\beta}+ \tau v)^2 \\
  %
  %
  %
  &= I[u] + \tau ( \int_{\Omega} \nabla u \cdot \nabla v + \kappa^{2}uv - fv + \int_{\partial \Omega} \beta( u - \frac{g}{\beta}) v ) +\mathbf{o} (\tau)\\
  % 
  %
  %
  &= I[u] + \tau ( \int_{\Omega} \nabla u \cdot \nabla v + \kappa^{2}uv - fv + \int_{\partial \Omega} (\beta u - g) v ) +\mathbf{o} (\tau).\\ 
\end{align*}

If $u$ is indeed a minimum for $I$ , then $i'(0) = 0$. The corresponding variational problem is:

\begin{align*}
\int_{\Omega} \nabla u \cdot \nabla v + \kappa^{2}uv + \int_{\partial \Omega} \beta u v  =\int_{\Omega} fv +\int_{\partial \Omega} gv. 
\end{align*}

Formally integrating the first term by parts gives:
$$
\int_{\Omega}  (-\Delta u + \kappa^{2} u) v + \int_{\partial \Omega} ( \beta u + \frac{ \partial u }{\partial n } )v = \int_{\Omega} fv + \int_{\partial \Omega} gv.
$$
We see the Robin condition arises naturally from the minimization of $I$. 


\section{Exact Solution in $d=1$ using Robin BC}
The problem we are dealing with can be solved exactly in 1D, if one chooses the right
Robin boundary condition.
Recall that the fundamental solution for $\Op = -\Delta + \kappa^{2}$ on $\mathbb{R}$ is $\Phi(x,y) = \frac{\exp( -\kappa |x-y| ) }{2\kappa}$.
Fix $x \in \Omega := (0,1)$ and define $H:=  \Phi - G$,  \cite{evans} calls this a corrector function. 
Consider a homogeneous Robin boundary condition
for $G$ on $\partial \Omega$ so that $\beta G + \frac{ \partial G}{\partial n_{y}} = 0$.
Once we plug in $H$, we see that it satisfies:

\begin{align}\label{difference equation}
  \begin{split} 
    (-\Delta_{y} + \kappa^2)H(x,y) &= 0, \forall y\in \Omega \\
    %
    %
    %     
    \beta H(x,y) + \frac{\partial H}{\partial n_{y}}(x,y) &= \beta \Phi(x,y) + \frac{\partial \Phi}{\partial n_{y}}(x,y), \forall y \in \partial \Omega.
  \end{split}
\end{align}
On $\partial \Omega = \{0,1\}$ we observe that

\begin{align*}
  \begin{split}
    \beta \Phi(0,x) + \frac{\partial \Phi}{\partial n_{y}}|_{y=0}(0,x) &= \frac{1}{2}\exp( -\kappa x)[ \frac{\beta}{\kappa} - 1] \\
    %
    %
    %
    \beta \Phi(1,x) + \frac{\partial \Phi}{\partial n_{y}}|_{y=1}(1,x) &= \frac{1}{2} \exp (-\kappa (1-x)) [\frac{\beta}{\kappa} - 1 ].\\
  \end{split}
\end{align*}

Setting $\beta = \kappa$, conclude that $H$ satisfies

\begin{align*}
  \begin{split}
    (-\Delta_{y} + \kappa^2 ) H(x,y) &= 0 , y\in \Omega \\
    % 
    % 
    % 
    \kappa H(x,y) + \frac{\partial H}{\partial n_{y}}(x,y) &= 0 , y\in \partial \Omega.\\
  \end{split}
\end{align*}

Thus, $\Phi - G = H \equiv 0$ and we have a perfect solution.

\section{Approximate Solution in $d >1$ using Homogeneous Robin BC}

In higher dimensions, we don't expect to have a perfect matching so we turn to approximations.
Denote $\Op$ the operator we use, with homogeneous Robin boundary condition. In this section, we may take 
$\Op_{1} = (-\Delta + \kappa^2)$ or $\Op_{2} = (-\Delta + \kappa^2)^2$, 
according to the dimension of our domain. If $d=2$, we may use either $\Op_{1}$ or $\Op_{2}$ (even though the
former does not make sense as a precision operator. When $d=3$ the integrals in the procedure below are infinite
(see subsection \ref{integrability} below), so we
may only use $\Op_{2}$ (and higher powers). Denote $\Phi$ the fundamental
solution for $\Op$. Fix $x\in \partial \Omega$. Denote, again $H = \Phi - G$. Equation \ref{difference equation}
still holds. If we can make $\beta H + \frac{ \partial H}{\partial n_{x}} =0$ then $H\equiv 0$ and
a perfect solution is found. Since we can't do that, we seek to minimize this rhs in \ref{difference equation},
for all $y\in \Omega$.

\begin{align}
  \begin{split}
    \beta^{*}(x) :&= \argmin_{\beta > 0} \frac{1}{2} \int_{\Omega} (\beta \Phi(x,y) + \frac{\partial \Phi}{\partial n_{x}} )^{2} dy \\
    % 
    % 
    % 
    &= \argmin_{\beta > 0} \frac{\beta^2}{2} \int_{\Omega} \Phi^2(x,y) dy+\beta \int_{\Omega} \Phi(x,y) \frac{\partial \Phi}{\partial n_{x}} dy. \\
  \end{split}
\end{align}

The idea is simple - instead of making the right hand side of the boundary condition zero, we seek to make it as small as possible for 
every $x\in \partial \Omega$. This quadratic has a minimum $\hat{\beta}(x) \in \mathbb{R}$ since its leading coefficient 
is positive. The minimum is found to be:

$$
\beta^{*}(x) = - \frac{\int_{\Omega}  \Phi(x,y) \frac{\partial \Phi}{\partial n_{x}} dy}{\int_{\Omega}  \Phi^2(x,y) dy}.
$$

% Recall that for a fixed $x$:

% $$
% \nabla_x\Phi(x,y) = -\frac{\kappa}{2\pi} K_1( \kappa ||x-y|| ) \frac{x-y}{||x-y||}.
% $$

\subsection{Integrability}\label{integrability}
Here we show that we can use the above formula for $\Op{1}$ when $d=2$ but not when $d=3$.
$\Op_{2}$ is OK for both, since it's green's function has no singularity in neither $d=2$ nor $d=3$.
Take WLOG $0=x\in \partial \Omega$ and consider half of the ball $B(0,\epsilon)$.
Since integtrability doesn't change if we take half of the ball or all of it, we just integrate in $y\in B(0,\epsilon)$. We take the boundary
of our domain s.t.  $\hat{n}_x = (-1,0,0)^t$.  If we take $\epsilon$ small enough, we can ignore $\kappa$, which will make the calculations
easier. Below, $C$ denotes a positive constant we don't care about. The constant can change between lines. 
Start with 3D problem and recall that in spherical coordinates $dy = r^2 \sin \theta dr d\theta d \phi$:

\begin{align*}
    \int_{B(0,\epsilon)} |m^2(y,0)|dy &= C\int_{B(0,\epsilon)} \frac{1}{||y||^2} dy \\
    &= C \int_{0}^{\epsilon} \frac{r^2}{r^2} dr \\
    &= C < \infty \\
    \int_{B(0,\epsilon)} |\Phi(0,y) \frac{\partial \Phi(0,y)}{\partial n_x}| dy &= C\int_{B(0,\epsilon)} |\frac{1}{||y||} (-\frac{1}{||y||^2} \frac{-y\cdot (-1,0,0)^t}{||y||})| dy\\
    &= C \int_{0}^{\epsilon} \frac{r }{r^4}r^2  dr  \\
    &= C \int_{0}^{\epsilon} \frac{1}{r} dr \\
    &= C \log r|_{r=0}^{r=\epsilon} \\
    & = \infty,
\end{align*}

Now the 2D problem. Recall that in polar coordinates $dy = r\sin \theta dr d\theta$:

\begin{align*}
     \int_{B(0,\epsilon)} |m^2(y,0)|dy &= C \int_{B(0,\epsilon)} \log^2||y|| dy \\
    &= C \int_{0}^{\epsilon} r \log^2 r dr \\
    &= C < \infty, \\
    \int_{B(0,\epsilon)} |\Phi(0,y) \frac{\partial \Phi(0,y)}{\partial n_x}| dy &= C \int_{B(0,\epsilon)} |\log||y|| (\frac{1}{||y||} \frac{-y\cdot (-1,0)^t}{||y||})| dy\\
    &= C\int_{0}^{\epsilon} |\log r\frac{r }{r^2}r| dr\\
    &= C \int_{0}^{\epsilon} |\log r| dr \\
    &= -C(r \log |r| - r)|_{0}^{\infty} < \infty.
\end{align*}

The above problem is uninformative in 3 dimensions but has a solution in 2 dimensions.

\section{Approximate solution in $d >1$ using inhomogeneous Robin BC}
If we allow a non-homogeneous Robin boundary condition, we make our covariance 
affine, instead of linear. This only changes the prior mean.

Just like in the precious section, consider a differential
 operator $\Op$, now with an inhomogeneous Robin boundary condition,
$ \beta u + \frac{\partial u}{\partial n} = g$. Fix $x\in \partial \Omega$ 
and let $H = \Phi - G$. Then $H$ satisfies

\begin{align*}
  &\Op H = 0 \text{ for } y \in  \Omega, \\
  &\beta H + \frac{\partial H}{\partial n_{y}} =  \beta\Phi  + \frac{\partial \Phi}{\partial n_{y}} - g  \text{ for } y \in \partial \Omega. 
\end{align*}

As before, we want $\beta(x) \Phi(x,y) + \frac{\partial \Phi}{\partial n_{x}}(x,y) - g(x) =0, \ \forall y \in \Omega$.
Again, we cannot satisfy this. So define, for scalars $\beta \in \mathbb{R}_{+},g\in \mathbb{R}$

$$
f(\beta, g;x) =\frac{1}{2} \int_{\Omega} ( \beta \Phi(x,y) + \frac{\partial \Phi}{\partial n_{x}}(x,y) - g)^{2} dy
$$

and minimize.

\begin{align}
  \begin{split}
    \beta^{*}(y), g^{*}(y) :&= \argmin_{g \in \mathbb{R}, \beta > 0} f(\beta,g ) \\
    %
    %
    %
    &=\frac{1}{2} \int_{\Omega} ( \beta \Phi(x,y) + \frac{\partial \Phi}{\partial n_{x}(x,y)} - g))^{2} dy \\
    % 
    % 
    % 
    &= \argmin_{g, \beta } \frac{1}{2} \int_{\Omega} ( \beta \Phi(x,y) + \nabla_{x} \Phi(x,y) \cdot n_{x} - g)^{2} dy\\
    % 
    % 
    &= \argmin_{g, \beta } \frac{1}{2} \int_{\Omega} \beta^2 \Phi^2(x,y) + (\nabla_{y} \Phi(x,y) \cdot n_{y})^2 + g^2\\
    &\ \ \ - 2g\beta\Phi(x,y) + 2\beta \Phi(x,y)\nabla_{x} \Phi(x,y) \cdot n_{x} \\
    &\ \ \ - 2g\nabla_{x} \Phi(x,y) \cdot n_{x} dy \\
    % 
    % 
    % 
    &= \argmin_{g, \beta } \frac{1}{2} \int_{\Omega} \beta^2 \Phi^2(x,y) +  g^2dy\\
    &\ \ \ + \int_{\Omega} \beta \Phi(x,y)\nabla_{x} \Phi(x,y) \cdot n_{x} -g\nabla_{x} \Phi(x,y) \cdot n_{x}\\
    &\ \ \ -g\beta\Phi(x,y) dy \\
    % 
    % 
    % 
  \end{split}
\end{align}

To minimize we differentiate.

\begin{align*}
  \frac{\partial f}{\partial \beta} &= \int_{\Omega} \beta\Phi^2(x,y) + \Phi(x,y)\nabla_{x} \Phi(x,y) \cdot n_{x} -g\Phi(x,y) dy\\
  %
  %
  %
  \frac{\partial f}{\partial g} &= \int_{\Omega} g - \nabla_{x} \Phi(x,y) \cdot n_{x} -\beta\Phi(x,y) dy. \\
\end{align*}

Equating these partial derivatives to zero results in a $2\times 2$ set of equations.

\[
\begin{bmatrix}
  \int_{\Omega}\Phi^2(x,y) dy & -\int_{\Omega} \Phi(x,y) dy \\
  -\int_{\Omega}\Phi(x,y) dy    & \int_{\Omega}dy \\
\end{bmatrix}
\begin{bmatrix}
  \beta^{*}(y) \\
  g^{*}(y) \\
\end{bmatrix}
=
\begin{bmatrix}
  -\int_{\Omega} \Phi(x,y) \nabla_{x}\Phi(x,y) \cdot n_{x} dy \\
  \int_{\Omega} \nabla_{x}\Phi(x,y) \cdot n_{x}dy
\end{bmatrix}.
\]

Now, all we need to do is decide on an operator. This will give us the $\Phi$ as a matern covariance and we will also
be able to differentiate it. 

\subsection{Calculation in $\mathbb{R}^2$}

According to \cite{stuart2010inverse} we must take at least $(-\Delta + \kappa^2)^{\nu + d/2}$ with $\nu > 0$. Here I take 
$\Op =(-\Delta + \kappa^2 )^2$. In this case, $\nu = 1$ and so using \ref{grad_matern}

\begin{align*}
  \Phi(x,y) &= \sigma^2 \kappa||x-y|| K_{1}(\kappa ||x-y||) \\
  \nabla_{x}\Phi(x,y) &= -\kappa  \sigma^2 \kappa||x-y|| K_{0}(\kappa ||x-y||) \frac{x-y}{||x-y||}.
\end{align*}

\section{A Newton's method approach}
In this section, two Newton's method based algorithms are suggested to solve the problem. 

\subsection{Reasoning and Intuition}
The covariance between two sites $x,y \in \mathbb{R}^d$ is not a very good measure for the dependence 
between them. If we denote the corresponding field by $Z( \cdot )$, then the covariance is 
defined by $\cov (x,y) =\mathbb{E}[Z(x) Z(y)]$ (assuming zero mean). If $\var(x)$ is big,
then so will be $\cov (x,y)$. Thus, it is better to consider correlations, since 
these are normalized: $\corr(x,y) = \frac{\cov (x,y)}{\sqrt{\var(x)\var(y)}}$. Equally useful
would be to ensure all variances are the same. My first goal, then, is to ensure
that all pointwise variances are equal: $\var(x) \equiv C \forall x \in \dom$ with $C \in \mathbb{R}$ a 
constant. If this holds,
then the covariances are on the same scale. We can then normalize easily, if we'd like to.

\subsection{Full Newton's Method}
Consider $\Op$, a precision operator and let $\mathcal{C}$ denote
its inverse. For concreteness,
consider $\Op = (-\Delta + \kappa)^2$ (defined without the
square on $\kappa$ for notational convenience). For ease of notation and 
understanding, discretize first. Then one has a mesh of
points $\{x_i\}_{i=1}^n \subseteq \Omega$ and a discretized
precision $\Op$ and covariance $\mathcal{C}$ (heavily abusing notation).
Define $f_{i}(\kappa) :=\log \var( x_i | \kappa)$. We 
are looking for a zero of this function. Note that we
have as many $\kappa^{(j)}$'s as there are $x_i$'s,
so we can expect to find a solution. Lets find the Jacobian.

\begin{align*}
  %
  %
  %
  [Df(\kappa)]_{ij} &= \frac{ \partial f_{i}(\kappa)}{\partial \kappa^{(j)}} \\
  %
  %
  %
  &= \frac{ \partial \log\var( x_i |\kappa)}{\partial \kappa^{(j)}} \\
  %
  %
  %
  %
  &= \frac{1}{\var(x_i|\kappa)} \frac{ \partial [\Op^{-1}]_{ii}}{\partial \kappa^{(j)}} \\
  %
  %
  % 
  &=  \frac{1}{\var(x_i|\kappa)}[-2\Op^{-1} (-\Delta + \kappa)\frac{ \partial \Op }{ \partial \kappa^{(j)} } \Op^{-1}]_{ii} \\ 
  %
  %
  %
  % 
  &=  \frac{1}{\var(x_i|\kappa)}[-2 (-\Delta + \kappa)^{-1}\frac{ \partial \Op }{ \partial \kappa^{(j)} } \Op^{-1}]_{ii} \\ 
  %
  %
  % 
  % 
  %
  %
  %
  &= -2\frac{1}{\var(x_i|\kappa)}[\mathcal{C}^{\frac{1}{2}} \frac{ \partial \diag(\kappa)}{\partial \kappa^{(j)}} \mathcal{C}]_{ii} \\
  %
  %
  %
  %
  %
  &= -2\frac{\mathcal{C}^{\frac{1}{2}}_{ij}\mathcal{C}_{ji}}{\var(x_i|\kappa)} \\
  % 
  % 
  %
  %
\end{align*}

We may use this in a Newton's iteration, $\kappa_{m+1} = \kappa_{m} + (Df)(\kappa_{m})^{-1}f(\kappa_{m})$.
However, there seems to be no way around calculating the Jacobi matrix. This would take $\mathcal{O}(n^2)$. Solving is even
worst. So we turn to a fast approximation.

\section{Sampling in FEM}

Whichever way we use to construct our covaraince, we are always faced with the task of samling. Here I show how this
can be done. The definitive property that I use is found in 
\cite{stuart2010inverse}, page 538. Assume $u \sim \mathcal{N}(0, \mathcal{C})$. Then

$$
\langle f , \mathcal{C} g \rangle = \mathbb{E}[\langle f,u\rangle \langle g,u \rangle].
$$

This is the property we want to keep in the finite dimensional case. Consider the Lagrange basis
functions $\phi_i, \phi_j \in V_{h}$,
the vector space of finite element functions. Consider also the discretized covariance operator
$C_{h}: V_h \to V_h$.% and its matrix representation $C \in \mathbb{R}^{n \times n}$.
In $V_h$, we have $u_{h}\sim \mathcal{N}(0,C_{h}), u_{h}(x) = \sum_{k}u_k\phi_{k}(x)$. Note that $u_{h} \in V_{h}$, whereas
$u = (u_k)_{k=1}^n \in \mathbb{R}^n$.

\begin{align*}
  \langle u_h , \phi_i \rangle &= \int \sum_{k} u_k \phi_k(x) \phi_i(x)dx\\
  %
  %
  %
  &= \sum u_k \int \phi_k\phi_i \\
  %
  %
  %
  %
  &= \sum u_k M_{ik} \\
  %
  %
  %
  &=(Mu)_i \\
  % 
  %
  %
  &\text{ and }\\ 
  % 
  % 
  % 
  \langle \phi_i, \mathcal{C} \phi_j \rangle  &= \langle \phi_{i} \mathcal{A}^{-2} \phi_{j} \rangle \\
  %
  %
  %
  &= \langle \phi_{i}, A^{-2}_{h} \phi_{j} \rangle \\ 
  %
  %
  &=\int \phi_{i}(x) (A^{-2}_{h} \phi_j)(x) dx \\
  %
  %
  %
  &=:[K^2]_{ij}.
\end{align*}

Consequently,

\begin{align*}
  K &= \mathbb{E}[ Mu (Mu)^t ]\\
  &= M \mathbb{E}[uu^t]M 
\end{align*}

and we conclude that $\mathbb{E}[uu^t] = M^{-1}KM^{-1}$. We can sample by taking $Z \sim \mathcal{N}(0,I)$
and set $u = M^{-1}K^{\frac{1}{2}}MZ$.











\section{Convexity}
It is good if the function $||var( \cdot | \kappa ) - 1 ||^2$ is convex. Let $\kappa = \frac{\kappa_1 + \kappa_2}{2}$



% \subsection{Cheap Newton's Method}
% Instead of using the full matrix $Df$, we may use only its diagonal. Let
% \[
% \widehat{Df}(\kappa)_{ij} :=
% \begin{cases}
%   - \var(x_i | \kappa ) & i = j\\
%   0 & o.w. 
% \end{cases}
% \]

% and the newton iteration is $\kappa^{(j)}_{m+1} = \kappa^{(j)}_{m}  + \frac{ \log \var( x_j| \kappa_{m} )}{ \var( x_j | \kappa_{m} ) }$.
% Pointwise variance can be easily estimated by sampling the gaussian $\mathcal{N}(0, \mathcal{C}(\kappa ))$,
% where $\mathcal{C}(\kappa)$  depends on $\kappa$.

% \section{Optimization Problem}
% Naturally, one can seek $\beta$ that minimizes the difference between the Green's function and
% the fundamental solution:

% $$
% \beta^{*} := \argmin_{\beta > 0} \frac{1}{2} \int_{\Omega}\int_{\Omega} (\Phi(x,y) - G(x,y))^2 dx dy
% $$

% But this looks hard. We can use what we have.
% to do this we start from a different spot. Say one has a finite element discretization of the problem. Then one may want to minimize the difference between
% the operators. This can be done as follows. Consider the finite element basis functions $\{f_y\}_{y \in S }$. In the current context, the easiest
% way to think of $f_{y}$ is as an approximation to $\delta_y$ (a dirac delta centered at $y\in S$). Note that to have a complete analogy, we must 
% have $\int_{\Omega}f_y = 1, \forall y \in S$.
% Now define:

% $$
% (-\Delta + \kappa^2)u_y = f_y \text{ in } \mathbb{R}^d
% $$

% and 

% \[
% \begin{cases}
%   (-\Delta + \kappa^2)v_y  &= f_y \text{ in } \Omega \\
%   \beta v_y + \frac{\partial v_y}{\partial n} &= 0 \text{ in } \partial \Omega . 
% \end{cases}
% \]

% Ideally, we'd like to have $u_y \equiv v_y, \ \forall y \in S$. We can't so we take

% $$
% \beta^{*} := \argmin_{\beta > 0} \frac{1}{2} \sum_{y \in S} \int_{\Omega}(u_y(x) - v_y(x) )^2dx.
% $$ 
% Now, take finer and finer discretizations. As $S \to \Omega$ (in the sense that $S$ becomes
% dense in $\Omega$), we get $f_y \to \delta_{y}, \ \sum_{y \in \Omega} \to \int_{\Omega}, \ u_y \to \Phi(\cdot, y)$
% and $v_y \to G(\cdot, y)$. Consequently,
% the optimization problem becomes

% $$
% \beta^{*} = \argmin_{\beta > 0} \frac{1}{2} \int_{\Omega}\int_{\Omega} (\Phi(x,y) - G(x,y) )^2 dx dy,
% $$

% which recovers the previous, more natural optimization problem.

\bibliographystyle{unsrt}
\bibliography{refs.bib}
\end{document}

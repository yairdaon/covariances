
\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{cite}
\usepackage{graphicx}
\usepackage{algorithm} % algorithm package
\usepackage[noend]{algpseudocode}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text





%----------------------------------------------------------------------------------------
% new commands
%----------------------------------------------------------------------------------------
\newcommand{\der}{\text{d}}
\newcommand{\coder}[1]{\texttt{#1}}
\newcommand{\inner}[2]{#1 \cdot #2}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{Corr}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\cov}{\text{Cov}}
\title{Title TBD}

\author{Yair Daon}
\date{}

\pdfinfo{%
  /Title    ()
  /Author   (Yair Daon)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\maketitle
\begin{abstract}
  I outline a method for solving my research problem (described below). I try to establish intuition
  using a discrete model. Surely most of the material here is not original. I just wanted to make the
  connection clear (at least for myself).
\end{abstract}

\section{The Problem}
Say you have a domain $\dom \subseteq \mathbb{R}^d$. One would like to consider a covariance operator on the domain 
that does not ``see'' the boundary. This means that there are no strong correlations / decorrelations close to
the boundary. The covariance operators that are considered in this context are ``laplacian-like operators''. Their properties
are studied by \cite{stuart2010inverse}.
If $d=2,3$ we take $\mathcal{C} = (-\Delta + \kappa(\cdot) )^{-2}$. If $d=1$, we can take 
$\mathcal{C} = (-\Delta + \kappa(\cdot) )^{-1}$.
If one takes Dirichlet boundary conditions, one obeserves 
strong decorrelations for points near the boundary. For Neumann boundary, the opposite occurs. We try 
to remove or ameliorate these boundary effects \cite{bui2013computational}.

\section{Reasoning and Intuition}
The covariance between two sites $x,y \in \mathbb{R}^d$ is not a very good measure for the dependence 
between them. If we denote the corresponding field by $Z( \cdot )$, then the covariance is 
defined by $\cov (x,y) =\mathbb{E}[Z(x) Z(y)]$ (assuming zero mean). If $\var(x)$ is big,
then so will be $\cov (x,y)$. Thus, it is better to consider correlations, since 
these are normalized: $\corr(x,y) = \frac{\cov (x,y)}{\sqrt{\var(x)\var(y)}}$. Equally useful
would be to ensure all variances are the same. My first goal, then, is to ensure
that all pointwise variances are equal: $\var(x) \equiv C \forall x \in \dom$ with $C \in \mathbb{R}$ a 
constant. If this holds,
then the covariances are on the same scale. We can then normalize easily, if we'd like to.

\section{Preliminaries}

\subsection{Continuous, Finite Difference and Graph Laplacians}
In this subsection a relation between the finite difference and the graph
laplacian is established.
First, I would like to make the connection between the discrete mass-laplacian and the
continuous mass-laplacian. I take the finite difference laplacian as an 
approximation of the continuous laplacian and consider $\dom =[0,1]$  
for now. Discretize by taking a mesh $M = \dom \cap \frac{1}{n} \mathbb{Z}$.
Consider the discrete transition probability matrix of a reflected simple random walk. 
Denote it $P$.

\[
P =
\begin{bmatrix}
  0            & 1            & 0           & 0              & \dots       & 0             & 0      &0            \\
  \frac{1}{2}  & 0            & \frac{1}{2} & 0              & \dots       & 0             & 0      &0            \\   
  0            & \frac{1}{2}  & 0           & \frac{1}{2}    & \dots       & 0             & 0      &0            \\ 
  \vdots       & \vdots       & \vdots      & \vdots         & \ddots      & \vdots        & \vdots &\vdots       \\
  0            & 0            & 0           &0               & \dots       & \frac{1}{2}   & 0      &\frac{1}{2}  \\
  0            & 0            & 0           &0               & \dots       & 0             & 1      &0            
\end{bmatrix}
\]

Denote $h = \frac{1}{n}$. The matirx of the finite difference
laplacian (with homogeneous Neumann boundary) is:

\[
\Delta_{FD} = \frac{1}{h^2}
\begin{bmatrix}
  -2           & 2           & 0           & 0              & \dots       & 0             & 0      &0            \\
  1            & -2           & 1           & 0              & \dots       & 0             & 0      &0            \\   
  0            & 1            & -2          & 1              & \dots       & 0             & 0      &0            \\ 
  \vdots       & \vdots       & \vdots      & \vdots         & \ddots      & \vdots        & \vdots &\vdots       \\
  0            & 0            & 0           &0               & \dots       & 1             & -2     &1            \\
  0            & 0            & 0           &0               & \dots       & 0             & 2      &-2           
\end{bmatrix}
\]

$-\Delta + \kappa \approx -[\Delta_{FD}] + \kappa$ holds in some suitable sense. Note
Clearly $h^2\Delta_{FD} = 2(P-I)$. Assume 
that now $\kappa \in \mathbb{R}^{n \times n}$ is a diagonal matrix such that 
$\kappa_{ii} =  \kappa(x_i)$.

\begin{align*}
  -[\Delta_{FD}] + \kappa  &= -\frac{2}{h^2}(P-I) + \kappa \\
  %
  %
  &= \frac{2}{h^2}[-(P-I) + \bar{\kappa}] \\
  %
  %
  %
  &=-\frac{2}{h^2}[(P- \bar{\kappa}) -I] \\
  %
  %
  %
  &=-\frac{2}{h^2}[P_{kill} -I], \\
  %
  %
  %
\end{align*}


where we denote

\begin{align}\label{kappas}
  \begin{split}
    \bar{\kappa} &= \frac{h^2\kappa}{2} \\
    P_{kill} &= P - \bar{\kappa}.
  \end{split}
\end{align}

We interpret $\bar{\kappa}_i$ as a killing probability at the the vertex $x_i$ identified
with the point $x_i = hi \in \dom$. This means we take $0 < \bar{\kappa} < 1$ and
consequently $ 0 < \kappa < \frac{2}{h^2}$. In the continuous case this restricion
does not apply - we may
take $\kappa$ as large as we want. However,
as $h \to 0$ we may take $\kappa$ larger and larger and in the limit this
restriction is removed. 

\subsection{Green's Functions}
In this subsection we define the Green's function using return counts and show
it is the right analogy to the continuous space Green's function.
Green's function is defined for the (killed and reflected) simple random walk on $M$ as:
\begin{align*}
  G_{ij}  &= \mathbb{E} [ \sum_{m=0}^{\infty} \mathbf{1}_{ \{x_j\} }( X^{x_i}_m)] \\
  %
  %
  %
  &= [\sum_{m=0}^{\infty} P_{kill}^m ]_{ij} \\
  %
  %
  %
  &= [(I - P_{kill})^{-1}]_{ij}.
\end{align*}

This relation holds since killing ensures $||P_{kill}|| < 1$. Given some $f \in C(\dom)$, we can 
solve the finite difference equation using the discrete Green's function. In the following derivation 
we restrict $f$ to the given mesh and extend $G$ to the domain $\dom$ as needed.

\begin{align*}
  u(x_i) :&=[( -\Delta_{FD} + \kappa )^{-1}f]_i \\
  %
  %
  % 
  &=\frac{h^2}{2}[(I - P_{kill})^{-1}f]_i \\ 
  %
  %
  %
  &=\frac{h^2}{2}[Gf]_i \\
  %
  %
  %
  &=\frac{h^2}{2}\sum_{j}G_{ij}f(x_j) \\
  %
  %
  %
  &\approx\int_{\dom}G(x_i,x)f(x)dx, \\
\end{align*}
where $G: \dom \times \dom \to \mathbb{R}$ is the Green's function for $-\Delta + \kappa$ with
homogeneous Neumann BC. The above derivation would
change for $\dom$ of higher dimensions - the 
exponent of $h$ absorbed in the integral is the dimensionality of $\dom$.
The continuous Green's function is thus approximated as

\begin{align}\label{G_cont}
  G(x_i,x_j) &\approx \frac{h}{2} G_{ij} 
\end{align}

\section{Newton's Method 1}
In this section I remove the bar from $\bar{\kappa}$
for convenience.
Define $f_{i}(\kappa) :=\var( x_i | \kappa) -1$. We 
are looking for a zero of this function. These are $n$ equations
in $n$ unknowns. Lets find the Jacobian.

\begin{align*}
  %
  %
  %
  [Df(\kappa)]_{ij} &= \frac{ \partial f_{i}(\kappa)}{\partial \kappa_j} \\
  %
  %
  %
  &= \frac{ \partial \var( x_i |\kappa)}{\partial \kappa_j} \\
  %
  %
  %
  %
  &= \frac{ [\partial (P - \kappa - I)^{-1}]_{ii}}{\partial \kappa_j} \\
  %
  %
  % 
  %
  %
  %
  &= [-(P-\kappa - I)^{-1} \frac{ \partial (P - \kappa - I)}{\partial \kappa_j} (P- \kappa - I)^{-1}]_{ii} \\
  %
  %
  % 
  % 
  %
  %
  %
  &= [G E^{jj}G]_{ii} \\
  %
  %
  %
  %
  %
  &= G_{ij}G_{ji} \\
  % 
  % 
  %
  %
  &= [G(\kappa) \circ G^t(\kappa) ]_{ij}
  % &= \sum_{s,t} (P-\kappa - I)_{is}E^{jj}_{st}(P- \kappa - I)_{ti} \\
  % %
  % %
  % %
  % %
  % &= (P-\kappa - I)_{ij}(P- \kappa - I)_{ji} \\
  % %
  % %
  % %
  % &=
  %   \begin{cases}
  %     (\kappa_i + 1)^2 & i = j\\
  %     P_{ij}P_{ji} & i \sim j \\
  %     0 & o.w.
  %   \end{cases} 
\end{align*}



\section{Newton's Method 2}
For this section, I take $\Delta :=\Delta_{FD}$, for 
ease of notation. Deonte $G(\kappa):= (-\Delta + \kappa )^{-1}$. 
Define $f_{i}(\kappa) :=G_{ii}(\kappa)$, which is interpreted
as $G_{ii}(\kappa) = \var( x_i | \kappa) -1$. We 
are looking for a zero of this function. These are $n$ equations
in $n$ unknowns. Lets find the Jacobian.

\begin{align*}
  %
  %
  %
  [Df(\kappa)]_{ij} &= \frac{ \partial f_{i}(\kappa)}{\partial \kappa_j} \\
  %
  %
  %
  &= \frac{ \partial G_{ii}(\kappa)}{\partial \kappa_j} \\
  %
  %
  %
  %
  &= \frac{ [\partial (-\Delta + \kappa)^{-1}]_{ii}}{\partial \kappa_j} \\
  %
  %
  % 
  %
  %
  %
  &= [-(-\Delta +\kappa )\frac{ \partial (-\Delta + \kappa)}{\partial \kappa_j}(-\Delta+ \kappa )]_{ii} \\
  %
  %
  % 
  % 
  %
  %
  %
  &= [(-\Delta  + \kappa)E^{jj}(-\Delta + \kappa)]_{ii} \\
  %
  %
  %
  %
  &= \sum_{s,t} (P-\kappa - I)_{is}E^{jj}_{st}(P- \kappa - I)_{ti} \\
  %
  %
  %
  %
  &= (P-\kappa - I)_{ij}(P- \kappa - I)_{ji} \\
  %
  %
  %
  &=
    \begin{cases}
      (\kappa_i + 1)^2 & i = j\\
      P_{ij}P_{ji} & i \sim j \\
      0 & o.w.
    \end{cases} 
\end{align*}

This matrix is sparse and can be stored in memory in $\mathcal{O}(n)$ space and solved 
in similar time.

 We can easily calculate 

$$
\kappa_{m+1} = \kappa_{m} - (Df(\kappa_{m}))^{-1} f(\kappa_{m})
$$

and hope for convergence.

\bibliography{refs.bib}
\bibliographystyle{unsrt}
\end{document}

